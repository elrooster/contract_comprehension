import boto3
import botocore
import time
import os
import os.path
import sys
import io
import urllib.parse
import json
import base64
import uuid
import gzip
import tarfile

from io import BytesIO
from s3transfer.manager import TransferManager
from requests_aws4auth import AWS4Auth
from opensearchpy import OpenSearch, RequestsHttpConnection

print('commencing more shenanigans')

####################################################################
### Global ###
region = 'us-east-1'
s3 = boto3.client('s3')
InputBucket = 'doc-repository-001'
StagingBucket = 'doc-repository-002'
OutputBucket = 'doc-repository-003'
JOB_ID = str(uuid.uuid1())
osClient = boto3.client('opensearch')
service = 'es'
host = 'search-document-search-ssbjzd4kon2gajx7wrs5pc7nm4.us-east-1.es.amazonaws.com'
credentials = boto3.Session().get_credentials()
comprehend_data_ARN = 'arn:aws:iam::926494140610:role/Comprehend_DataRole'

####################################################################
### functions ###

### Connect to the opensearch domain
def connectOS():
    print('connecting to the OS Endpoint {0}')
    awsauth = AWS4Auth(credentials.access_key,
    credentials.secret_key,
    region, service,
    session_token = credentials.token)
    print(awsauth)

    try:
        es = OpenSearch(
            hosts = [{'host': host, 'port': 443}],
            http_auth = awsauth,
            use_ssl = True,
            verify_certs = True,
            connection_class = RequestsHttpConnection)
        print('connected to OS')
        return es

    except Exception as E:
        print('unable to connect to {0}')
        print(E)
        exit(3)


#################################################################
def lambda_handler(event, context):
    print('received event: ' + json.dumps(event, indent=2))

    # Get object from event, show content
    OutputBucket = event['Records'][0]['s3']['bucket']['name']
    ObjectName = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding = 'utf-8')

    s3Uri_txt = 's3://'+OutputBucket+'//'+ObjectName
    print('Object Name: ' + ObjectName)
    print('Input Bucket: ' + OutputBucket)
    print('s3URI_txt: ' + s3Uri_txt)
    original_ObjectName = ObjectName[4:-4]
    print('s3URI_pdf: ' + 's3://' + InputBucket + '//' + original_ObjectName)

    #Clean up comprehend output
    print(ObjectName)
    output = s3.get_object(Bucket = OutputBucket, Key = ObjectName)
    with gzip.GzipFile(fileobj=output.get('Body')) as gzipfile:
        content = gzipfile.read()
    print(type(content))
    entities = content.decode('utf-8')
    entities = ''.join(entities.split())
    start = entities.find('{')
    end = entities.rfind('}')+1
    entities = entities[start:end]
    entities_json = json.loads(entities)
    print(entities_json)

    StagingObject = entities_json.get('File')
    print(type(StagingObject))
    StagingObject = '/txt/' + StagingObject
    print(StagingBucket)
    print(StagingObject)
    text = s3.get_object(Bucket = StagingBucket, Key = StagingObject)
    print(type(text))
    print(text)

    InputObject = StagingObject[5:-4]
    print(InputObject)
    InputURL =  'https://s3.console.aws.amazon.com/s3/object/' + InputBucket + '/' + InputObject + '?region=' + region
    print(InputURL)



        ## Write to OpenSearch
        # searchdata={'s3link':s3url,'text':text}
        # print(searchdata)
        # print('connecting to opensearch')
        # es = connectOS()
        # es.index(index = 'document', doc_type = '_doc', body = searchdata)
        # print('data uploaded to opensearch')
        # return 'victory?'

    # except Exception as e:
    #     print(e)
    #     print('Error getting object {} from bucket {}')
    #     raise e
