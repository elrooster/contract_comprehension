import boto3
import botocore
import time
import os
import os.path
import sys
import io
import urllib.parse
import json
import requests
import base64

from io import BytesIO
from requests_aws4auth import AWS4Auth
from opensearchpy import OpenSearch, RequestsHttpConnection
from s3transfer.manager import TransferManager
from trp import Document


print('commencing shenanigans')

####################################################################
### Global ###
region = 'us-east-1'
s3 = boto3.client('s3')
textract = boto3.client(service_name = 'textract', region_name = region)
InputBucket = 'doc-repository-001'
StagingBucket = 'doc-repository-002'
OutputBucket = 'doc-repository-003'
osClient = boto3.client('opensearch')
service = 'es'
host = 'search-document-search-ssbjzd4kon2gajx7wrs5pc7nm4.us-east-1.es.amazonaws.com'
credentials = boto3.Session().get_credentials()

####################################################################
### functions ###

### Textract: Start Async Job
def startJob(inputbucket, objectname):
    response = None
    response = textract.start_document_text_detection(
    DocumentLocation={
        'S3Object': {
            'Bucket': inputbucket,
            'Name': objectname
        }
    })

    return response['JobId']

### Textract: Check job
def isJobComplete(jobid):
    time.sleep(5)
    response = textract.get_document_text_detection(JobId=jobid)
    status = response["JobStatus"]
    print("Job status: {}".format(status))

    while(status == "IN_PROGRESS"):
        time.sleep(5)
        response = textract.get_document_text_detection(JobId=jobid)
        status = response["JobStatus"]
        print("Job status: {}".format(status))

    return status

### Textract: Get job
def getJobResults(jobid):
    pages = []
    time.sleep(5)
    response = textract.get_document_text_detection(JobId=jobid)
    pages.append(response)
    print("Resultset page recieved: {}".format(len(pages)))
    nextToken = None
    if('NextToken' in response):
        nextToken = response['NextToken']
    while(nextToken):
        time.sleep(5)
        response = textract.get_document_text_detection(JobId=jobid, NextToken=nextToken)
        pages.append(response)
        print("Resultset page recieved: {}".format(len(pages)))
        nextToken = None
        if('NextToken' in response):
            nextToken = response['NextToken']
    return pages

#################################################################
def lambda_handler(event, context):
    print('received event: ' + json.dumps(event, indent=2))

    # Get object from event, show content
    InputBucket = event['Records'][0]['s3']['bucket']['name']
    ObjectName = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding = 'utf-8')
    s3Uri = 's3://'+InputBucket+'/'+ObjectName
    print('Object Name: ' + ObjectName)
    print('Input Bucket: ' + InputBucket)
    print('s3URI: '+ s3Uri)

    try:
        print(s3Uri)
        JobId = startJob(InputBucket, ObjectName)
        print('Job Started')

        #loop until job complete
        if(isJobComplete(JobId)):
            response = getJobResults(JobId)

        save json response from textract into s3
        raw_textract = s3.put_object(
            Bucket = StagingBucket,
            Key = '/json/' + ObjectName + '-RAW.json',
            Body = json.dumps(response)
        )
        print(f'Textracted: {StagingBucket}/{ObjectName}.json')

        #recreate text from textract Output
        text=''
        for result in response:
            for block in result["Blocks"]:
                if block["BlockType"] == "LINE":
                    text += block['Text'] + " "

        save text file into bucket
        txt_textract = s3.put_object(
            Bucket = StagingBucket,
            Key = '/txt/' + ObjectName + '-TEXT.txt',
            Body = json.dumps(text)
        )
        print(f'Text file created: {StagingBucket}/{ObjectName}.txt')

    except Exception as e:
        print(e)
        print('Error getting object {} from bucket {}')
        raise e
